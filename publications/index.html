<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | J. Lu</title> <meta name="author" content="J. Lu"/> <meta name="description" content="Authors marked by * contributed equally."/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ‘“</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ludles.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">J.Â <span class="font-weight-bold">Lu</span> <div class="navbar-brand social"> Â <a href="mailto:%6C%75@%64%69.%6B%75.%64%6B" title="email"><i class="fas fa-envelope-square"></i></a> <a href="https://github.com/ludles" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github-square"></i></a> <a href="https://orcid.org/0000-0002-1309-5294" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid-square"></i></a> <a href="https://scholar.google.com/citations?user=4BbYWywAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar-square"></i></a> <a href="https://www.linkedin.com/in/-lu-" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">Authors marked by * contributed equally.</p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <abbr class="badge">CVPR</abbr> <span class="badge award">Poster</span> </div> </div> </div> <div id="chongXfibrosis2024" class="col-sm-8"> <div class="title">XFibrosis: Explicit Vessel-Fiber Modeling for Fibrosis Staging from Liver Pathology Images</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=mIlgsX4AAAAJ" target="_blank" rel="noopener noreferrer">C. Yin</a>,Â S. Liu,Â F. Lyu,Â <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=j04-HfIAAAAJ" target="_blank" rel="noopener noreferrer">S. Darkner</a>,Â V. W. Wong,Â andÂ P. C. Yuen</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR), </em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>The increasing prevalence of non-alcoholic fatty liver disease (NAFLD) has caused public concern in recent years. Fibrosis staging from liver biopsy images plays a key role in demonstrating the histological progression of NAFLD. Fibrosis mainly involves the deposition of fibers around vessels. Current deep learning-based fibrosis staging methods learn spatial relationships between tissue patches but do not explicitly consider the relationships between vessels and fibers, leading to limited performance and poor interpretability. In this paper, we propose an eXplicit vessel-fiber modeling method for Fibrosis staging from liver biopsy images, namely XFibrosis. Specifically, we transform vessels and fibers into graph-structured representations, where their micro-structures are depicted by vessel-induced primal graphs and fiber-induced dual graphs, respectively. Moreover, the fiber-induced dual graphs also represent the connectivity information between vessels caused by fiber deposition. A primal-dual graph convolution module is designed to facilitate the learning of spatial relationships between vessels and fibers, allowing for the joint exploration and interaction of their micro-structures. Experiments conducted on two datasets have shown that explicitly modeling the relationship between vessels and fibers leads to improved fibrosis staging and enhanced interpretability.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge">SIIMS</abbr></div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="https://d3i71xaburhd42.cloudfront.net/2ab3149508d9a297f9901301b087de02c83b0db4/20-Figure8-1.png"> </div> </div> </div> <div id="baoSlidingFirstOrder2023" class="col-sm-8"> <div class="title">Sliding at First Order: Higher-Order Momentum Distributions for Discontinuous Image Registration</div> <div class="author"> L. Bao,Â <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=dU6ePjIAAAAJ" target="_blank" rel="noopener noreferrer">S. Ying</a>,Â andÂ <a href="https://scholar.google.com/citations?hl=en&amp;user=GGh0haAAAAAJ" target="_blank" rel="noopener noreferrer">S. Sommer</a> </div> <div class="periodical"> <em>SIAM Journal on Imaging Sciences, </em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2103.16262" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/baolily/hokreg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In this paper, we propose a new approach to deformable image registration that captures sliding motions. The large deformation diffeomorphic metric mapping (LDDMM) registration method faces challenges in representing sliding motion since it per construction generates smooth warps. To address this issue, we extend LDDMM by incorporating both zeroth- and first-order momenta with a non-differentiable kernel. This allows to represent both discontinuous deformation at switching boundaries and diffeomorphic deformation in homogeneous regions. We provide a mathematical analysis of the proposed deformation model from the viewpoint of discontinuous systems. To evaluate our approach, we conduct experiments on both artificial images and the publicly available DIR-Lab 4DCT dataset. Results show the effectiveness of our approach in capturing plausible sliding motion.</p> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <abbr class="badge">ISBI</abbr> <span class="badge award">Oral</span> </div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="https://raw.githubusercontent.com/diku-dk/credanno/09a737fc0d417e31982d98e059bea3693d508047/imgs/cRedAnno_plus_Intro.svg"> </div> </div> </div> <div id="luCRedAnnoAnnotationExploitation2023" class="col-sm-8"> <div class="title">cRedAnno+: Annotation Exploitation in Self-Explanatory Lung Nodule Diagnosis</div> <div class="author"> <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=mIlgsX4AAAAJ" target="_blank" rel="noopener noreferrer">C. Yin</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=CQkvlpUAAAAJ" target="_blank" rel="noopener noreferrer">K. Erleben</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=9gTyw1wAAAAJ" target="_blank" rel="noopener noreferrer">M. B. Nielsen</a>,Â andÂ <a href="https://scholar.google.com/citations?hl=en&amp;user=j04-HfIAAAAJ" target="_blank" rel="noopener noreferrer">S. Darkner</a> </div> <div class="periodical"> <em>In IEEE International Symposium on Biomedical Imaging (ISBI), </em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2210.16097" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1109/ISBI53787.2023.10230720" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10230720" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/diku-dk/credanno" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster_credanno_ISBI2023.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides_credanno_ISBI2023.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Recently, attempts have been made to reduce annotation requirements in feature-based self-explanatory models for lung nodule diagnosis. As a representative, cRedAnno achieves competitive performance with considerably reduced annotation needs by introducing self-supervised contrastive learning to do unsupervised feature extraction. However, it exhibits unstable performance under scarce annotation conditions. To improve the accuracy and robustness of cRedAnno, we propose an annotation exploitation mechanism by conducting semi-supervised active learning in the learned semantically meaningful space to jointly utilise the extracted features, annotations, and unlabelled data. The proposed approach achieves comparable or even higher malignancy prediction accuracy with 10x fewer annotations, meanwhile showing better robustness and nodule attribute prediction accuracy. Our complete code is open-source available: https://github.com/diku-dk/credanno.</p> </div> </div> </div> </li></ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <abbr class="badge">iMIMIC</abbr> <span class="badge award">Oral</span> </div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/luReducingAnnotationNeed2022.svg"> </div> </div> </div> <div id="luReducingAnnotationNeed2022" class="col-sm-8"> <div class="title">Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis</div> <div class="author"> <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=mIlgsX4AAAAJ" target="_blank" rel="noopener noreferrer">C. Yin</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=5ER3NYoAAAAJ" target="_blank" rel="noopener noreferrer">O. Krause</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=CQkvlpUAAAAJ" target="_blank" rel="noopener noreferrer">K. Erleben</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=9gTyw1wAAAAJ" target="_blank" rel="noopener noreferrer">M. B. Nielsen</a>,Â andÂ <a href="https://scholar.google.com/citations?hl=en&amp;user=j04-HfIAAAAJ" target="_blank" rel="noopener noreferrer">S. Darkner</a> </div> <div class="periodical"> <em>In Workshop on Interpretability of Machine Intelligence in Medical Image Computing (iMIMIC) at MICCAI, </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2206.13608" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1007/978-3-031-17976-1_4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-17976-1_4.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/diku-dk/credanno" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster_credanno.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides_credanno_MICCAI2022.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Feature-based self-explanatory methods explain their classification in terms of human-understandable features. In the medical imaging community, this semantic matching of clinical knowledge adds significantly to the trustworthiness of the AI. However, the cost of additional annotation of features remains a pressing issue. We address this problem by proposing cRedAnno, a data-/annotation-efficient self-explanatory approach for lung nodule diagnosis. cRedAnno considerably reduces the annotation need by introducing self-supervised contrastive learning to alleviate the burden of learning most parameters from annotation, replacing end-to-end training with two-stage training. When training with hundreds of nodule samples and only 1% of their annotations, cRedAnno achieves competitive accuracy in predicting malignancy, meanwhile significantly surpassing most previous works in predicting nodule attributes. Visualisation of the learned space further indicates that the correlation between the clustering of malignancy and nodule attributes coincides with clinical knowledge. Our complete code is open-source available: https://github.com/diku-dk/credanno. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge">PLOSONE</abbr></div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/luPanacea2021_thumb.png"> </div> </div> </div> <div id="luPanacea2021" class="col-sm-8"> <div class="title">Is Image-to-Image Translation the Panacea for Multimodal Image Registration? A Comparative Study</div> <div class="author"> <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Ã–fverstedt</a>,Â <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,Â andÂ <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>PLOS ONE, </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2103.16262" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1371/journal.pone.0276196" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/MIDA-group/MultiRegEval" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Despite current advancement in the field of biomedical image processing, propelled by the deep learning revolution, multimodal image registration, due to its several challenges, is still often performed manually by specialists. The recent success of image-to-image (I2I) translation in computer vision applications and its growing use in biomedical areas provide a tempting possibility of transforming the multimodal registration problem into a, potentially easier, monomodal one. We conduct an empirical study of the applicability of modern I2I translation methods for the task of multimodal biomedical image registration. We compare the performance of four Generative Adversarial Network (GAN)-based methods and one contrastive representation learning method, subsequently combined with two representative monomodal registration methods, to judge the effectiveness of modality translation for multimodal image registration. We evaluate these method combinations on three publicly available multimodal datasets of increasing difficulty, and compare with the performance of registration by Mutual Information maximisation and one modern data-specific multimodal registration method. Our results suggest that, although I2I translation may be helpful when the modalities to register are clearly correlated, registration of modalities which express distinctly different properties of the sample are not well handled by the I2I translation approach. When less information is shared between the modalities, the I2I translation methods struggle to provide good predictions, which impairs the registration performance. The evaluated representation learning method, which aims to find an in-between representation, manages better, and so does the Mutual Information maximisation approach. We share our complete experimental setup as open-source (https://github.com/MIDA-group/MultiRegEval).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge"><a href="https://ssba.org.se/" target="_blank" rel="noopener noreferrer">SSBA</a></abbr></div> </div> </div> <div id="WetzerRotationallyEquivariantRepresentation2022" class="col-sm-8"> <div class="title">Rotationally Equivariant Representation Learning for Multimodal Images</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=qSc3kA8AAAAJ" target="_blank" rel="noopener noreferrer">E. Wetzer</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=MmqXB5oAAAAJ" target="_blank" rel="noopener noreferrer">N. Pielawski</a>,Â <a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Ã–fverstedt</a>,Â <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=17soDRoAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">C. WÃ¤hlby</a>,Â <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,Â andÂ <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>In Swedish Society for Automated Image Analysis (SSBA) Symposium on Image Analysis, </em> 2022 </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge"><a href="https://www.comulis.eu/" target="_blank" rel="noopener noreferrer">COMULIS</a></abbr></div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="https://www.it.uu.se/research/visual_information_and_interaction/research/mida/GAN.png"> </div> </div> </div> <div id="luImagetoImage2021" class="col-sm-8"> <div class="title">Image-to-Image Translation in Multimodal Image Registration : How Well does It Work?</div> <div class="author"> <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Ã–fverstedt</a>,Â <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,Â andÂ <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>In COMULIS Conference, </em> 2021 </div> <div class="links"> <a href="https://cci-gothenburg.github.io/LuJ" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.it.uu.se/research/visual_information_and_interaction/research/mida/ComulisLu.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <span class="badge award">Dataset</span> </div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/luDatasetsEvaluationMultimodal2021_thumb.png"> </div> </div> </div> <div id="luDatasetsEvaluationMultimodal2021" class="col-sm-8"> <div class="title">Datasets for Evaluation of Multimodal Image Registration</div> <div class="author"> <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Ã–fverstedt</a>,Â <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,Â andÂ <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>DOI: 10.5281/zenodo.5557568, </em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.5281/zenodo.5557568" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>In total, it contains 864 image pairs created from the aerial dataset, 5040 image pairs created from the cytological dataset, 536 image pairs created from the histological dataset, and metadata with scripts to create the 480 volume pairs from the radiological dataset. Each image pair consists of a reference patch I_Ref and its corresponding initial transformed patch I_Init in both modalities, along with the ground-truth transformation parameters to recover it.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge"><a href="https://www.comulis.eu/" target="_blank" rel="noopener noreferrer">COMULIS</a></abbr></div> </div> </div> <div id="WetzerRegistrationMultimodal2021" class="col-sm-8"> <div class="title">Registration of Multimodal Microscopy Images using CoMIR - Learned Structural Image Representations</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=qSc3kA8AAAAJ" target="_blank" rel="noopener noreferrer">E. Wetzer</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=MmqXB5oAAAAJ" target="_blank" rel="noopener noreferrer">N. Pielawski</a>,Â <a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Ã–fverstedt</a>,Â <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=17soDRoAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">C. WÃ¤hlby</a>,Â <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,Â andÂ <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>In COMULIS Conference, </em> 2021 </div> <div class="links"> <a href="https://cci-gothenburg.github.io/WetzerE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.it.uu.se/research/visual_information_and_interaction/research/mida/ComulisElisabeth.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <span class="badge award">Workshop</span> </div> </div> </div> <div id="WetzerContrastiveLearning2021" class="col-sm-8"> <div class="title">Contrastive Learning for Equivariant Multimodal Image Representations</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=qSc3kA8AAAAJ" target="_blank" rel="noopener noreferrer">E. Wetzer</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=MmqXB5oAAAAJ" target="_blank" rel="noopener noreferrer">N. Pielawski</a>,Â E. Breznik,Â <a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Ã–fverstedt</a>,Â <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=17soDRoAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">C. WÃ¤hlby</a>,Â <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,Â andÂ <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>In The Power of Women in Deep Learning Workshop at the Mathematics of Deep Learning Programme at the Isaac Newton Institute, </em> 2021 </div> <div class="links"> <a href="https://www.it.uu.se/research/visual_information_and_interaction/research/mida/WDL21_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <abbr class="badge">ICIAR</abbr> <span class="badge award">Oral</span> </div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="https://media.springernature.com/full/springer-static/image/chp%3A10.1007%2F978-3-030-50516-5_22/MediaObjects/500864_1_En_22_Fig1_HTML.png?as=webp"> </div> </div> </div> <div id="luDeepLearningBased2020" class="col-sm-8"> <div class="title">A Deep Learning Based Pipeline for Efficient Oral Cancer Screening on Whole Slide Images</div> <div class="author"> <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a>,Â C. Runow Stark,Â E. Darai Ramqvist,Â J-M. Hirsch,Â andÂ <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a> </div> <div class="periodical"> <em>In International Conference on Image Analysis and Recognition (ICIAR), </em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/1910.10549" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1007/978-3-030-50516-5_22" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/MIDA-group/OralScreen" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Oral cancer incidence is rapidly increasing worldwide. The most important determinant factor in cancer survival is early diagnosis. To facilitate large scale screening, we propose a fully automated pipeline for oral cancer detection on whole slide cytology images. The pipeline consists of fully convolutional regression-based nucleus detection, followed by per-cell focus selection, and CNN based classification. Our novel focus selection step provides fast per-cell focus decisions at human-level accuracy. We demonstrate that the pipeline provides efficient cancer classification of whole slide cytology images, improving over previous results both in terms of accuracy and feasibility. The complete source code is made available as open source ( https://github.com/MIDA-group/OralScreen ).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge">NeurIPS</abbr></div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="https://github.com/MIDA-group/CoMIR/raw/master/resources/comir_pipeline.jpg"> </div> </div> </div> <div id="pielawskiCoMIRContrastiveMultimodal2020" class="col-sm-8"> <div class="title">CoMIR: Contrastive Multimodal Image Representation for Registration</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=MmqXB5oAAAAJ" target="_blank" rel="noopener noreferrer">N. Pielawski</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=qSc3kA8AAAAJ" target="_blank" rel="noopener noreferrer">E. Wetzer</a>,Â <a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Ã–fverstedt</a>,Â <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=17soDRoAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">C. WÃ¤hlby</a>,Â <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,Â andÂ <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS), </em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2006.06325" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://proceedings.neurips.cc/paper/2020/hash/d6428eecbe0f7dff83fc607c5044b2b9-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://proceedings.neurips.cc/paper/2020/file/d6428eecbe0f7dff83fc607c5044b2b9-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/MIDA-group/CoMIR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.it.uu.se/research/visual_information_and_interaction/research/mida/NeurIPS2020.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> <a href="https://slideslive.com/38937317" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> </div> <div class="abstract hidden"> <p>We propose contrastive coding to learn shared, dense image representations, referred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs enable the registration of multimodal images where existing registration methods often fail due to a lack of sufficiently similar image structures. CoMIRs reduce the multimodal registration problem to a monomodal one, in which general intensity-based, as well as feature-based, registration algorithms can be applied. The method involves training one neural network per modality on aligned images, using a contrastive loss based on noise-contrastive estimation (InfoNCE). Unlike other contrastive coding methods, used for, e.g., classification, our approach generates image-like representations that contain the information shared between modalities. We introduce a novel, hyperparameter-free modification to InfoNCE, to enforce rotational equivariance of the learnt representations, a property essential to the registration task. We assess the extent of achieved rotational equivariance and the stability of the representations with respect to weight initialization, training set, and hyperparameter settings, on a remote sensing dataset of RGB and near-infrared images. We evaluate the learnt representations through registration of a biomedical dataset of bright-field and second-harmonic generation microscopy images; two modalities with very little apparent correlation. The proposed approach based on CoMIRs significantly outperforms registration of representations created by GAN-based image-to-image translation, as well as a state-of-the-art, application-specific method which takes additional knowledge about the data into account. Code is available at: https://github.com/MIDA-group/CoMIR.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge">MSc Thesis</abbr></div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/MultimodalUU_thumb.jpg"> </div> </div> </div> <div id="luEvaluationLearningbasedMethods2020" class="col-sm-8"> <div class="title">Evaluation of Learning-based Methods for Multimodal Biomedical Image Registration</div> <div class="author"> <em>J. Lu</em> </div> <div class="periodical"> <em>Master Programme in Computational Science, </em> 2020 </div> <div class="links"> <a href="http://uu.diva-portal.org/smash/record.jsf?pid=diva2:1602468" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="http://uu.diva-portal.org/smash/get/diva2:1602468/FULLTEXT01.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge"><a href="https://eubias.org/NEUBIAS/" target="_blank" rel="noopener noreferrer">NEUBIAS</a></abbr></div> </div> </div> <div id="WetzerCrossmodalRepresentation2020" class="col-sm-8"> <div class="title">Cross-modal Representation Learning for Efficient Registration of Multiphoton and Brightfield Microscopy Images of Skin Tissue</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=qSc3kA8AAAAJ" target="_blank" rel="noopener noreferrer">E. Wetzer</a>,Â <a href="https://scholar.google.com/citations?hl=en&amp;user=MmqXB5oAAAAJ" target="_blank" rel="noopener noreferrer">N. Pielawski</a>,Â <a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Ã–fverstedt</a>,Â <em>J. Lu</em>,Â <a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,Â I. Floroiu,Â A. Dumitru,Â M. Costache,Â R. Hristu,Â S.G. Stanciu,Â andÂ <a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>In NEUBIAS Conference, </em> 2020 </div> <div class="links"> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2024 J. Lu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: 07 March, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NXMTTJ2GJ4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NXMTTJ2GJ4");</script> </body> </html>