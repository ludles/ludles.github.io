<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>J. Lu</title> <meta name="author" content="J. Lu"/> <meta name="description" content="Lu's website "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üëì</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ludles.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> J. <span class="font-weight-bold">Lu</span> </h1> <p class="desc"><b>PhD Candidate</b> @ <a href="https://www.ku.dk/english/" target="_blank" rel="noopener noreferrer">University of Copenhagen</a></p> </header> <article> <div class="profile float-right" style="max-width: 30vw"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile_img-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile_img-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile_img-1400.webp"></source> <img src="/assets/img/profile_img.jpeg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="profile_img.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <i class="fas fa-map-marker-alt"></i>¬† Montr√©al, Canada </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6C%75@%64%69.%6B%75.%64%6B" title="email"><i class="fas fa-envelope-square"></i></a> <a href="https://github.com/ludles" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github-square"></i></a> <a href="https://orcid.org/0000-0002-1309-5294" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid-square"></i></a> <a href="https://scholar.google.com/citations?user=4BbYWywAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar-square"></i></a> <a href="https://www.linkedin.com/in/-lu-" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> Email is the best way to reach me. </div> </div> </div> <div class="clearfix"> <p>Hey, thanks for stopping by! üëã</p> <p>I am pursuing a PhD at the <a href="https://di.ku.dk/english/research/image/" target="_blank" rel="noopener noreferrer">IMAGE section</a> of the <a href="https://di.ku.dk/english/" target="_blank" rel="noopener noreferrer">Department of Computer Science</a>, <a href="https://www.ku.dk/english/" target="_blank" rel="noopener noreferrer">University of Copenhagen</a>, co-supervised by <a href="https://di.ku.dk/english/staff/vip/researchers_image/?pure=en/persons/383640" target="_blank" rel="noopener noreferrer">Prof Sune Darkner</a> and <a href="https://research.regionh.dk/rigshospitalet/da/persons/michael-bachmann-nielsen(87d575e5-755e-4182-b94d-75776981fc21).html" target="_blank" rel="noopener noreferrer">Prof Michael Bachmann Nielsen</a>.</p> <p>My PhD research aims to develop trustworthy AI for medical imaging, where trustworthiness focuses on Explainable AI (XAI) and also extends to human-in-the-loop learning and uncertainty quantification. In collaboration with my fantastic colleagues, I also dabble in image registration, segmentation and classification.</p> <p>I am currently having my visiting stay at the <a href="https://ismart.ece.mcgill.ca/" target="_blank" rel="noopener noreferrer">iSMART Lab</a>, <a href="https://www.mcgill.ca/" target="_blank" rel="noopener noreferrer">McGill University</a>, under the supervision of <a href="https://ismart.ece.mcgill.ca/team/" target="_blank" rel="noopener noreferrer">Prof Narges Armanfard</a>, extending my interest in XAI to anomaly detection.</p> <p>Previously, I attained my MSc in Computational Science at the <a href="https://www.it.uu.se/" target="_blank" rel="noopener noreferrer">Department of Information Technology</a>, <a href="https://uu.se/en/" target="_blank" rel="noopener noreferrer">Uppsala University</a>. There I had the pleasure of working in the <a href="https://www.it.uu.se/research/visual_information_and_interaction/research/mida" target="_blank" rel="noopener noreferrer">MIDA group</a>, under the supervision of <a href="https://www.cb.uu.se/~natasa/" target="_blank" rel="noopener noreferrer">Prof Nata≈°a Sladoje</a> and <a href="https://www.cb.uu.se/~joakim/" target="_blank" rel="noopener noreferrer">Prof Joakim Lindblad</a>.</p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive" style="max-height: 10vw; min-height: 10em"> <table class="table table-sm table-borderless"> <tr> <th scope="row" class="th-sm">08 Sep, 2023</th> <td> Congratulations to my MSc students <a href="https://www.linkedin.com/in/jiashuo-li-376a13231/" target="_blank" rel="noopener noreferrer">Jiashuo Li</a> and <a href="https://www.linkedin.com/in/xinyi-huang-384364231/" target="_blank" rel="noopener noreferrer">Xinyi Huang</a> for successfully defending their thesis with the highest grade (12/12). </td> </tr> <tr> <th scope="row" class="th-sm">29 Jun, 2023</th> <td> Congratulations to my MSc students <a href="https://www.linkedin.com/in/bin-zhang-a4a435236/" target="_blank" rel="noopener noreferrer">Bin Zhang</a> and <a href="https://www.linkedin.com/in/yufei-yuan-904760235/" target="_blank" rel="noopener noreferrer">Yufei Yuan</a> for successfully defending their thesis with the highest grade (12/12). </td> </tr> <tr> <th scope="row" class="th-sm">15 Apr, 2023</th> <td> I relocated to the <a href="https://ismart.ece.mcgill.ca/" target="_blank" rel="noopener noreferrer">iSMART Lab</a> at <a href="https://www.mcgill.ca/" target="_blank" rel="noopener noreferrer">McGill University</a> for a research stay, supervised by <a href="https://ismart.ece.mcgill.ca/team/" target="_blank" rel="noopener noreferrer">Prof Narges Armanfard</a>. </td> </tr> <tr> <th scope="row" class="th-sm">23 Jan, 2023</th> <td> Our manuscript ‚Äú<a href="https://arxiv.org/abs/2210.16097" target="_blank" rel="noopener noreferrer">cRedAnno+</a>‚Äù is accepted. Meet me at <a href="https://2023.biomedicalimaging.org/en/" target="_blank" rel="noopener noreferrer">ISBI 2023</a>. ü•Ç </td> </tr> <tr> <th scope="row" class="th-sm">29 Sep, 2022</th> <td> Meet me at <a href="https://responsibleml4healthcare.github.io/" target="_blank" rel="noopener noreferrer">Workshop on Responsible Machine Learning in Healthcare</a>. </td> </tr> <tr> <th scope="row" class="th-sm">26 Aug, 2022</th> <td> My simple little website is revamped and online. üéä </td> </tr> <tr> <th scope="row" class="th-sm">19 Jul, 2022</th> <td> Our manuscript ‚Äú<a href="https://arxiv.org/abs/2206.13608" target="_blank" rel="noopener noreferrer">Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis</a>‚Äù is accepted. Meet me at <a href="https://conferences.miccai.org/2022/en/" target="_blank" rel="noopener noreferrer">MICCAI 2022</a>. ü•Ç </td> </tr> <tr> <th scope="row" class="th-sm">01 Jul, 2022</th> <td> Meet me at <a href="https://human-in-the-loop.compute.dtu.dk/" target="_blank" rel="noopener noreferrer">Summer school on human-in-the-loop and learning with limited labels</a>. </td> </tr> <tr> <th scope="row" class="th-sm">30 Aug, 2021</th> <td> Meet me at <a href="https://www.comulis.eu/comulis-conference-2021" target="_blank" rel="noopener noreferrer">COMULIS 2021</a>. </td> </tr> <tr> <th scope="row" class="th-sm">25 Nov, 2020</th> <td> Meet me at <a href="https://nips.cc/Conferences/2020" target="_blank" rel="noopener noreferrer">NeurIPS 2020</a>. </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected publications</h2> <p>Find the full list <a href="publications">here</a>.</p> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <abbr class="badge">CVPR</abbr> <span class="badge award">Poster</span> </div> </div> </div> <div id="chongXfibrosis2024" class="col-sm-8"> <div class="title">XFibrosis: Explicit Vessel-Fiber Modeling for Fibrosis Staging from Liver Pathology Images</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=mIlgsX4AAAAJ" target="_blank" rel="noopener noreferrer">C. Yin</a>,¬†S. Liu,¬†F. Lyu,¬†<em>J. Lu</em>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=j04-HfIAAAAJ" target="_blank" rel="noopener noreferrer">S. Darkner</a>,¬†V. W. Wong,¬†and¬†P. C. Yuen</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR), </em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>The increasing prevalence of non-alcoholic fatty liver disease (NAFLD) has caused public concern in recent years. Fibrosis staging from liver biopsy images plays a key role in demonstrating the histological progression of NAFLD. Fibrosis mainly involves the deposition of fibers around vessels. Current deep learning-based fibrosis staging methods learn spatial relationships between tissue patches but do not explicitly consider the relationships between vessels and fibers, leading to limited performance and poor interpretability. In this paper, we propose an eXplicit vessel-fiber modeling method for Fibrosis staging from liver biopsy images, namely XFibrosis. Specifically, we transform vessels and fibers into graph-structured representations, where their micro-structures are depicted by vessel-induced primal graphs and fiber-induced dual graphs, respectively. Moreover, the fiber-induced dual graphs also represent the connectivity information between vessels caused by fiber deposition. A primal-dual graph convolution module is designed to facilitate the learning of spatial relationships between vessels and fibers, allowing for the joint exploration and interaction of their micro-structures. Experiments conducted on two datasets have shown that explicitly modeling the relationship between vessels and fibers leads to improved fibrosis staging and enhanced interpretability.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <abbr class="badge">ISBI</abbr> <span class="badge award">Oral</span> </div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="https://raw.githubusercontent.com/diku-dk/credanno/09a737fc0d417e31982d98e059bea3693d508047/imgs/cRedAnno_plus_Intro.svg"> </div> </div> </div> <div id="luCRedAnnoAnnotationExploitation2023" class="col-sm-8"> <div class="title">cRedAnno+: Annotation Exploitation in Self-Explanatory Lung Nodule Diagnosis</div> <div class="author"> <em>J. Lu</em>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=mIlgsX4AAAAJ" target="_blank" rel="noopener noreferrer">C. Yin</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=CQkvlpUAAAAJ" target="_blank" rel="noopener noreferrer">K. Erleben</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=9gTyw1wAAAAJ" target="_blank" rel="noopener noreferrer">M. B. Nielsen</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=j04-HfIAAAAJ" target="_blank" rel="noopener noreferrer">S. Darkner</a> </div> <div class="periodical"> <em>In IEEE International Symposium on Biomedical Imaging (ISBI), </em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2210.16097" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1109/ISBI53787.2023.10230720" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10230720" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/diku-dk/credanno" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster_credanno_ISBI2023.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides_credanno_ISBI2023.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Recently, attempts have been made to reduce annotation requirements in feature-based self-explanatory models for lung nodule diagnosis. As a representative, cRedAnno achieves competitive performance with considerably reduced annotation needs by introducing self-supervised contrastive learning to do unsupervised feature extraction. However, it exhibits unstable performance under scarce annotation conditions. To improve the accuracy and robustness of cRedAnno, we propose an annotation exploitation mechanism by conducting semi-supervised active learning in the learned semantically meaningful space to jointly utilise the extracted features, annotations, and unlabelled data. The proposed approach achieves comparable or even higher malignancy prediction accuracy with 10x fewer annotations, meanwhile showing better robustness and nodule attribute prediction accuracy. Our complete code is open-source available: https://github.com/diku-dk/credanno.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <abbr class="badge">iMIMIC</abbr> <span class="badge award">Oral</span> </div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/luReducingAnnotationNeed2022.svg"> </div> </div> </div> <div id="luReducingAnnotationNeed2022" class="col-sm-8"> <div class="title">Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis</div> <div class="author"> <em>J. Lu</em>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=mIlgsX4AAAAJ" target="_blank" rel="noopener noreferrer">C. Yin</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=5ER3NYoAAAAJ" target="_blank" rel="noopener noreferrer">O. Krause</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=CQkvlpUAAAAJ" target="_blank" rel="noopener noreferrer">K. Erleben</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=9gTyw1wAAAAJ" target="_blank" rel="noopener noreferrer">M. B. Nielsen</a>,¬†and¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=j04-HfIAAAAJ" target="_blank" rel="noopener noreferrer">S. Darkner</a> </div> <div class="periodical"> <em>In Workshop on Interpretability of Machine Intelligence in Medical Image Computing (iMIMIC) at MICCAI, </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2206.13608" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1007/978-3-031-17976-1_4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-17976-1_4.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/diku-dk/credanno" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster_credanno.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/slides_credanno_MICCAI2022.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Feature-based self-explanatory methods explain their classification in terms of human-understandable features. In the medical imaging community, this semantic matching of clinical knowledge adds significantly to the trustworthiness of the AI. However, the cost of additional annotation of features remains a pressing issue. We address this problem by proposing cRedAnno, a data-/annotation-efficient self-explanatory approach for lung nodule diagnosis. cRedAnno considerably reduces the annotation need by introducing self-supervised contrastive learning to alleviate the burden of learning most parameters from annotation, replacing end-to-end training with two-stage training. When training with hundreds of nodule samples and only 1% of their annotations, cRedAnno achieves competitive accuracy in predicting malignancy, meanwhile significantly surpassing most previous works in predicting nodule attributes. Visualisation of the learned space further indicates that the correlation between the clustering of malignancy and nodule attributes coincides with clinical knowledge. Our complete code is open-source available: https://github.com/diku-dk/credanno. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge">PLOSONE</abbr></div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/luPanacea2021_thumb.png"> </div> </div> </div> <div id="luPanacea2021" class="col-sm-8"> <div class="title">Is Image-to-Image Translation the Panacea for Multimodal Image Registration? A Comparative Study</div> <div class="author"> <em>J. Lu</em>,¬†<a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. √ñfverstedt</a>,¬†<a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>PLOS ONE, </em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2103.16262" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1371/journal.pone.0276196" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/MIDA-group/MultiRegEval" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Despite current advancement in the field of biomedical image processing, propelled by the deep learning revolution, multimodal image registration, due to its several challenges, is still often performed manually by specialists. The recent success of image-to-image (I2I) translation in computer vision applications and its growing use in biomedical areas provide a tempting possibility of transforming the multimodal registration problem into a, potentially easier, monomodal one. We conduct an empirical study of the applicability of modern I2I translation methods for the task of multimodal biomedical image registration. We compare the performance of four Generative Adversarial Network (GAN)-based methods and one contrastive representation learning method, subsequently combined with two representative monomodal registration methods, to judge the effectiveness of modality translation for multimodal image registration. We evaluate these method combinations on three publicly available multimodal datasets of increasing difficulty, and compare with the performance of registration by Mutual Information maximisation and one modern data-specific multimodal registration method. Our results suggest that, although I2I translation may be helpful when the modalities to register are clearly correlated, registration of modalities which express distinctly different properties of the sample are not well handled by the I2I translation approach. When less information is shared between the modalities, the I2I translation methods struggle to provide good predictions, which impairs the registration performance. The evaluated representation learning method, which aims to find an in-between representation, manages better, and so does the Mutual Information maximisation approach. We share our complete experimental setup as open-source (https://github.com/MIDA-group/MultiRegEval).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"> <abbr class="badge">ICIAR</abbr> <span class="badge award">Oral</span> </div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="https://media.springernature.com/full/springer-static/image/chp%3A10.1007%2F978-3-030-50516-5_22/MediaObjects/500864_1_En_22_Fig1_HTML.png?as=webp"> </div> </div> </div> <div id="luDeepLearningBased2020" class="col-sm-8"> <div class="title">A Deep Learning Based Pipeline for Efficient Oral Cancer Screening on Whole Slide Images</div> <div class="author"> <em>J. Lu</em>,¬†<a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a>,¬†C. Runow Stark,¬†E. Darai Ramqvist,¬†J-M. Hirsch,¬†and¬†<a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a> </div> <div class="periodical"> <em>In International Conference on Image Analysis and Recognition (ICIAR), </em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/1910.10549" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1007/978-3-030-50516-5_22" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/MIDA-group/OralScreen" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Oral cancer incidence is rapidly increasing worldwide. The most important determinant factor in cancer survival is early diagnosis. To facilitate large scale screening, we propose a fully automated pipeline for oral cancer detection on whole slide cytology images. The pipeline consists of fully convolutional regression-based nucleus detection, followed by per-cell focus selection, and CNN based classification. Our novel focus selection step provides fast per-cell focus decisions at human-level accuracy. We demonstrate that the pipeline provides efficient cancer classification of whole slide cytology images, improving over previous results both in terms of accuracy and feasibility. The complete source code is made available as open source ( https://github.com/MIDA-group/OralScreen ).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 no-gutters"> <div class="row no-gutters"> <div class="col abbr"><abbr class="badge">NeurIPS</abbr></div> </div> <div class="row no-gutters"> <div class="col preview"> <img class="preview z-depth-1 rounded" src="https://github.com/MIDA-group/CoMIR/raw/master/resources/comir_pipeline.jpg"> </div> </div> </div> <div id="pielawskiCoMIRContrastiveMultimodal2020" class="col-sm-8"> <div class="title">CoMIR: Contrastive Multimodal Image Representation for Registration</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=MmqXB5oAAAAJ" target="_blank" rel="noopener noreferrer">N. Pielawski</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=qSc3kA8AAAAJ" target="_blank" rel="noopener noreferrer">E. Wetzer</a>,¬†<a href="https://scholar.google.com/citations?user=GMminVMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. √ñfverstedt</a>,¬†<em>J. Lu</em>,¬†<a href="https://scholar.google.com/citations?user=17soDRoAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">C. W√§hlby</a>,¬†<a href="https://scholar.google.com/citations?user=BtuFYvQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">J. Lindblad</a>,¬†and¬†<a href="https://scholar.google.com/citations?user=n4uDNF8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">N. Sladoje</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS), </em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2006.06325" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://proceedings.neurips.cc/paper/2020/hash/d6428eecbe0f7dff83fc607c5044b2b9-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://proceedings.neurips.cc/paper/2020/file/d6428eecbe0f7dff83fc607c5044b2b9-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/MIDA-group/CoMIR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.it.uu.se/research/visual_information_and_interaction/research/mida/NeurIPS2020.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> <a href="https://slideslive.com/38937317" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> </div> <div class="abstract hidden"> <p>We propose contrastive coding to learn shared, dense image representations, referred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs enable the registration of multimodal images where existing registration methods often fail due to a lack of sufficiently similar image structures. CoMIRs reduce the multimodal registration problem to a monomodal one, in which general intensity-based, as well as feature-based, registration algorithms can be applied. The method involves training one neural network per modality on aligned images, using a contrastive loss based on noise-contrastive estimation (InfoNCE). Unlike other contrastive coding methods, used for, e.g., classification, our approach generates image-like representations that contain the information shared between modalities. We introduce a novel, hyperparameter-free modification to InfoNCE, to enforce rotational equivariance of the learnt representations, a property essential to the registration task. We assess the extent of achieved rotational equivariance and the stability of the representations with respect to weight initialization, training set, and hyperparameter settings, on a remote sensing dataset of RGB and near-infrared images. We evaluate the learnt representations through registration of a biomedical dataset of bright-field and second-harmonic generation microscopy images; two modalities with very little apparent correlation. The proposed approach based on CoMIRs significantly outperforms registration of representations created by GAN-based image-to-image translation, as well as a state-of-the-art, application-specific method which takes additional knowledge about the data into account. Code is available at: https://github.com/MIDA-group/CoMIR.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 J. Lu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: 07 March, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NXMTTJ2GJ4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NXMTTJ2GJ4");</script> </body> </html>